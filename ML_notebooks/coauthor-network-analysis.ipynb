{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:58:51.650889Z","iopub.execute_input":"2022-03-09T15:58:51.651389Z","iopub.status.idle":"2022-03-09T15:58:51.6964Z","shell.execute_reply.started":"2022-03-09T15:58:51.651306Z","shell.execute_reply":"2022-03-09T15:58:51.695437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nWe will use Deep Walk (which is a concept based of Word Embeddings) to cluster author networks.","metadata":{}},{"cell_type":"code","source":"! pip install pyvis","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:59:17.908762Z","iopub.execute_input":"2022-03-09T15:59:17.909232Z","iopub.status.idle":"2022-03-09T15:59:28.710506Z","shell.execute_reply.started":"2022-03-09T15:59:17.909196Z","shell.execute_reply":"2022-03-09T15:59:28.709327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom datetime import datetime\nimport sys\nimport ast\n\nimport plotly.express as px\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport networkx as nx\nfrom networkx.algorithms.components.connected import connected_components\n\nimport json\nimport dask.bag as db\n\nimport sys\nimport os\n\nsys.path.append(\"..\")\n\nfrom pathlib import Path\n\nimport json\n\n\nfrom itertools import combinations\nfrom collections import Counter\nfrom itertools import chain\nimport random\n\nfrom tqdm.notebook import tqdm, trange\nimport time    # to be used in loop iterations\n\nimport multiprocessing\nimport smart_open\n\nfrom gensim.models.word2vec import Word2Vec\n\nfrom pyvis.network import Network\n\nfrom IPython.core.display import display, HTML\n\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity,cosine_distances\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:59:28.712465Z","iopub.execute_input":"2022-03-09T15:59:28.712824Z","iopub.status.idle":"2022-03-09T15:59:29.323982Z","shell.execute_reply.started":"2022-03-09T15:59:28.712763Z","shell.execute_reply":"2022-03-09T15:59:29.3225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract the Data from Kaggle ","metadata":{}},{"cell_type":"code","source":"# Extract Only the AI , ML PAPERS\ndef extractArxivData(categories=['stat.ML','cs.AI'],year=None,raw_data_path=\"../data/raw/\",save_extracted_filename=\"../data/processed/AI_ML.json\"):\n    \"\"\" This function extracts data for the given set of categories and save the data into the save_extracted_filename path \"\"\"\n    records=db.read_text(raw_data_path+\"/*.json\").map(lambda x:json.loads(x))\n    docs = (records.filter(lambda x:any(ele in x['categories'] for ele in categories)==True))\n    extract_latest_version=lambda x:x['versions'][-1][\"created\"]\n    if year!=None:\n        docs=docs.filter(lambda x:int(extract_latest_version(x).split(\" \")[3])>=year)\n\n    get_metadata = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'],\n                  'abstract':x['abstract'],\n                 'version':x['versions'][-1]['created'],\n                         'doi':x[\"doi\"],\n                         'authors_parsed':x['authors_parsed']}\n                        \n    data=docs.map(get_metadata).to_dataframe().compute()\n\n    ## Creating authors fields by joining first and last nmes in authors_parsed columns.\n    data['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\n\n    print(\"Number of Records Extracted for Given Set of Categories \",data.shape[0])\n    Path(os.path.dirname(save_extracted_filename)).mkdir(parents=True, exist_ok=True)\n    data.to_json(save_extracted_filename,orient=\"records\")\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:59:29.326074Z","iopub.execute_input":"2022-03-09T15:59:29.326617Z","iopub.status.idle":"2022-03-09T15:59:29.391149Z","shell.execute_reply.started":"2022-03-09T15:59:29.326583Z","shell.execute_reply":"2022-03-09T15:59:29.390233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RAW_DATA_PATH=\"../input/arxiv/\"\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:08:52.726938Z","iopub.execute_input":"2022-03-09T16:08:52.727319Z","iopub.status.idle":"2022-03-09T16:08:52.778196Z","shell.execute_reply.started":"2022-03-09T16:08:52.727287Z","shell.execute_reply":"2022-03-09T16:08:52.776821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Collect data for Papers published in ['stat.ML','cs.AI'] since year 2015.\ndata=extractArxivData(categories=['stat.ML','cs.AI'],year=2015,raw_data_path=RAW_DATA_PATH,save_extracted_filename=\"AI_ML_since2015.json\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:09:08.969535Z","iopub.execute_input":"2022-03-09T16:09:08.969912Z","iopub.status.idle":"2022-03-09T16:10:27.428159Z","shell.execute_reply.started":"2022-03-09T16:09:08.969881Z","shell.execute_reply":"2022-03-09T16:10:27.426512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a Co-Author Network\n\nFor the set of papers extracted, for every pair of authors an edge is to be created. The Edge weight will be the number of papers the two authors have collabrated on. ","metadata":{}},{"cell_type":"markdown","source":"## Load the Data\n","metadata":{}},{"cell_type":"code","source":"data['author_pairs']=data['authors'].apply(lambda x:list(combinations(x, 2)))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:17.488671Z","iopub.execute_input":"2022-03-09T12:33:17.4891Z","iopub.status.idle":"2022-03-09T12:33:17.769204Z","shell.execute_reply.started":"2022-03-09T12:33:17.489055Z","shell.execute_reply":"2022-03-09T12:33:17.768194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We consider authors who have published papers after 2015 and published more than 3 papers.\n","metadata":{}},{"cell_type":"code","source":"def flattenList(nested_list):\n    flat_list = [item for sublist in nested_list for item in sublist]\n    return flat_list","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:17.772296Z","iopub.execute_input":"2022-03-09T12:33:17.772787Z","iopub.status.idle":"2022-03-09T12:33:17.820713Z","shell.execute_reply.started":"2022-03-09T12:33:17.772744Z","shell.execute_reply":"2022-03-09T12:33:17.819681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ai_authors=pd.DataFrame(flattenList(data['authors'].tolist())).rename(columns={0:'authors'})\npapers_by_authors=ai_authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False)\npapers_by_authors.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:17.822919Z","iopub.execute_input":"2022-03-09T12:33:17.823262Z","iopub.status.idle":"2022-03-09T12:33:18.389582Z","shell.execute_reply.started":"2022-03-09T12:33:17.823223Z","shell.execute_reply":"2022-03-09T12:33:18.388547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers_by_authors['Number of Papers Published'].describe()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:18.391091Z","iopub.execute_input":"2022-03-09T12:33:18.39145Z","iopub.status.idle":"2022-03-09T12:33:18.455727Z","shell.execute_reply.started":"2022-03-09T12:33:18.391417Z","shell.execute_reply":"2022-03-09T12:33:18.454681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Keeping Authors who have published more than 3 Papers\nnodes_to_keep=papers_by_authors.loc[papers_by_authors['Number of Papers Published']>3,'authors'].tolist()\nlen(nodes_to_keep)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:18.456981Z","iopub.execute_input":"2022-03-09T12:33:18.457293Z","iopub.status.idle":"2022-03-09T12:33:18.514177Z","shell.execute_reply.started":"2022-03-09T12:33:18.457264Z","shell.execute_reply":"2022-03-09T12:33:18.512547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(nodes_to_keep)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T13:24:50.799479Z","iopub.execute_input":"2022-03-09T13:24:50.799932Z","iopub.status.idle":"2022-03-09T13:24:50.882574Z","shell.execute_reply.started":"2022-03-09T13:24:50.799823Z","shell.execute_reply":"2022-03-09T13:24:50.88078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating the Edges of the Co-Author Network","metadata":{}},{"cell_type":"code","source":"authors_pairs=data['author_pairs'].tolist()\nauthors_edge_list=[item for sublist in authors_pairs for item in sublist]\nauthors_weighted_edge_list=list(Counter(authors_edge_list).items())\nauthors_weighted_edge_list=[(row[0][0],row[0][1],row[1]) for idx,row in enumerate(authors_weighted_edge_list)]\nauthors_weighted_edge_list[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:18.515865Z","iopub.execute_input":"2022-03-09T12:33:18.516316Z","iopub.status.idle":"2022-03-09T12:33:19.590706Z","shell.execute_reply.started":"2022-03-09T12:33:18.516269Z","shell.execute_reply":"2022-03-09T12:33:19.589873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Graph on the Complete Data","metadata":{}},{"cell_type":"code","source":"G1=nx.Graph()\nG1.add_weighted_edges_from(authors_weighted_edge_list)\nprint(len(G1.nodes()))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:19.591889Z","iopub.execute_input":"2022-03-09T12:33:19.592206Z","iopub.status.idle":"2022-03-09T12:33:21.346164Z","shell.execute_reply.started":"2022-03-09T12:33:19.592175Z","shell.execute_reply":"2022-03-09T12:33:21.345017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filtering the Graph, to keep nodes (authors) who have atleast published 4 papers. We also remove any isolated nodes.","metadata":{"execution":{"iopub.status.busy":"2021-06-10T17:22:51.692696Z","iopub.execute_input":"2021-06-10T17:22:51.693185Z","iopub.status.idle":"2021-06-10T17:22:51.744902Z","shell.execute_reply.started":"2021-06-10T17:22:51.693141Z","shell.execute_reply":"2021-06-10T17:22:51.744116Z"}}},{"cell_type":"code","source":"## From the complete Graph, create a subgraph, with only the nodes to keep\nsub_g=nx.subgraph(G1,nodes_to_keep)\nG=nx.Graph(sub_g)\nprint(len(G.nodes()))\nisolated_node=nx.isolates(G)\nlen(list(isolated_node))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:21.347707Z","iopub.execute_input":"2022-03-09T12:33:21.348044Z","iopub.status.idle":"2022-03-09T12:33:23.53534Z","shell.execute_reply.started":"2022-03-09T12:33:21.348011Z","shell.execute_reply":"2022-03-09T12:33:23.534133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G.remove_nodes_from(list(nx.isolates(G)))\nlen(G.nodes)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:23.53682Z","iopub.execute_input":"2022-03-09T12:33:23.537174Z","iopub.status.idle":"2022-03-09T12:33:23.604631Z","shell.execute_reply.started":"2022-03-09T12:33:23.537139Z","shell.execute_reply":"2022-03-09T12:33:23.603487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del G1, sub_g","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:23.605722Z","iopub.execute_input":"2022-03-09T12:33:23.60601Z","iopub.status.idle":"2022-03-09T12:33:23.818206Z","shell.execute_reply.started":"2022-03-09T12:33:23.605983Z","shell.execute_reply":"2022-03-09T12:33:23.816932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Nodes in Author Graph \",len(G.nodes()))\nprint(\"Number of Edges in AUthor Graph \",len(G.edges()))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:23.81983Z","iopub.execute_input":"2022-03-09T12:33:23.820308Z","iopub.status.idle":"2022-03-09T12:33:23.878714Z","shell.execute_reply.started":"2022-03-09T12:33:23.820269Z","shell.execute_reply":"2022-03-09T12:33:23.877895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing Deep Walk\n\n**Deep walk uses the concept of Random Walks to assign an embedding to each node in the network.** \n\n1. In Random Walk, given a node we pick one of its neighbours at random and move to this node and from this node again choose another node among its neighbours at random. This continues for a fixed number of steps. \n\n\n\n2. Once we have random walks generated for every node in the network, in DeepWalk the next step is to predict probability of visiting node \"v\" on a random walk starting from node \"u\". \n \n3. This is very similar to the Skip-Gram model used in Word2Vec Model in NLP, wherein we try to predict the neighbouring words given a particular target word.","metadata":{}},{"cell_type":"code","source":"def getRandomWalk(graph,node,length_of_random_walk):\n    \"\"\" This function takes NetworkX Graph and a Node and generate random walk for a given length \n    \n    Returns the random walk (list of nodes traversed)\n\n    Note: The same node may occcur more than once in a Random Walk.\n    \"\"\"\n    start_node=node\n    current_node=start_node\n    random_walk=[node]\n    for i in range(0,length_of_random_walk):\n        ## Choose a random neighbour of the current node\n        \n        current_node_neighbours=list(graph.neighbors(current_node))\n        chosen_node=random.choice(current_node_neighbours)\n        current_node=chosen_node\n        random_walk.append(current_node)\n    return random_walk\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:23.87998Z","iopub.execute_input":"2022-03-09T12:33:23.880303Z","iopub.status.idle":"2022-03-09T12:33:23.928222Z","shell.execute_reply.started":"2022-03-09T12:33:23.880273Z","shell.execute_reply":"2022-03-09T12:33:23.927251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### For every Node in the Graph, get randomwalks . For eahc node, let us get random walks say around 10 times each of path length 10\nnum_sampling= 10\nrandom_walks=[]\nlength_of_random_walk= 3\nfor node in tqdm(G.nodes(),desc=\"Iterating Nodes\"):\n    for i in range(0,num_sampling):\n        random_walks.append(getRandomWalk(G,node,length_of_random_walk))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-03-09T12:33:23.929176Z","iopub.execute_input":"2022-03-09T12:33:23.929444Z","iopub.status.idle":"2022-03-09T12:33:26.400975Z","shell.execute_reply.started":"2022-03-09T12:33:23.929418Z","shell.execute_reply":"2022-03-09T12:33:26.399905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data now is similar to list of words in a sentence and we can use gensim to create Node Embedding Model - here each author is a Node and Node is similar to word in a sentence","metadata":{}},{"cell_type":"code","source":"deepwalk_model=Word2Vec(sentences=random_walks,window=5,sg=1,negative=5,vector_size=128,epochs= 20,compute_loss=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:33:26.402578Z","iopub.execute_input":"2022-03-09T12:33:26.403187Z","iopub.status.idle":"2022-03-09T12:34:00.328671Z","shell.execute_reply.started":"2022-03-09T12:33:26.40314Z","shell.execute_reply":"2022-03-09T12:34:00.327703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deepwalk_model.save(\"deepwalk_since2015.model\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:34:00.330052Z","iopub.execute_input":"2022-03-09T12:34:00.330337Z","iopub.status.idle":"2022-03-09T12:34:00.417863Z","shell.execute_reply.started":"2022-03-09T12:34:00.330309Z","shell.execute_reply":"2022-03-09T12:34:00.416778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similar authors ","metadata":{}},{"cell_type":"code","source":"def getSimilarNodes(model,node):\n    \"\"\"\n    This function takes deepwalk model and a node\n    \n    Returns the top 10 nodes (author) similar to the given node \n    \"\"\"\n    similarity=model.wv.most_similar(node)\n    similar_nodes=pd.DataFrame()\n    similar_nodes['Similar_Node']=[row[0] for i,row in enumerate(similarity)]\n    similar_nodes['Similarity_Score']=[row[1] for i,row in enumerate(similarity)]\n    similar_nodes['Source_Node']=node\n    return similar_nodes\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:34:00.419412Z","iopub.execute_input":"2022-03-09T12:34:00.41984Z","iopub.status.idle":"2022-03-09T12:34:00.470625Z","shell.execute_reply.started":"2022-03-09T12:34:00.419795Z","shell.execute_reply":"2022-03-09T12:34:00.469563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getSimilarNodes(deepwalk_model,\"Bengio Yoshua\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T12:34:00.473823Z","iopub.execute_input":"2022-03-09T12:34:00.474179Z","iopub.status.idle":"2022-03-09T12:34:00.547855Z","shell.execute_reply.started":"2022-03-09T12:34:00.474145Z","shell.execute_reply":"2022-03-09T12:34:00.54673Z"},"trusted":true},"execution_count":null,"outputs":[]}]}